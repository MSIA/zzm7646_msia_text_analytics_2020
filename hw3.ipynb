{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/zazhu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libiaries\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import json\n",
    "import multiprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import joblib, pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_yelp_line(line):\n",
    "    # conver the text line to a json object\n",
    "    json_object = json.loads(line)\n",
    "    \n",
    "    # read and tokenize the text\n",
    "    text = json_object['text']\n",
    "    \n",
    "    # read the label and convert to an integer\n",
    "    label = int(json_object['stars'])\n",
    "    \n",
    "    # return the text and the label\n",
    "    if text:\n",
    "        return text, label\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first 500,000 yelp reviews\n",
    "lines = open('yelp_academic_dataset_review.json', encoding=\"utf8\").readlines()[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribute the processing across the machine cpus\n",
    "pool = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "result = pool.map(process_yelp_line, lines)\n",
    "result = list(filter(None, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"unzip\" the (tokens, label) tuples to a list of lists of tokens, and a list of labels\n",
    "texts, labels = zip(*result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# convert to dataframe\n",
    "data = pd.DataFrame({'text': texts, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only numbers, letters and space\n",
    "data['text'] = data.apply(lambda t: re.sub(r'[^0-9A-Za-z ]', '', str(t['text'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and convert to lower case\n",
    "data['text'] = data.apply(lambda r: ' '.join(w.lower() for w in r['text'].split() if w.lower() not in stop_words),axis=1)\n",
    "\n",
    "# discard NA reviews\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign binary labels - 1 for rating>=3: good rating\n",
    "data['binary_label'] = data.apply(lambda r: 0 if r['label'] < 4 else 1, axis=1)\n",
    "data.to_csv('review_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"review_cleaned.csv\").drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>someone worked many museums eager visit galler...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actually horrified place still business 3 year...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love deagans really atmosphere cozy festive sh...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dismal lukewarm defrostedtasting texmex glopmu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh happy day finally canes near casa yes other...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>kung fu tea havent amazing boba long time firs...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>wish one westside case boba shoplet start taro...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>new favorite spot dylan waitress absolutely fa...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>35 starsenvironment decor poor floor sometimes...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>want tell everyone experience tuesday evening ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  binary_label\n",
       "0       someone worked many museums eager visit galler...      2             0\n",
       "1       actually horrified place still business 3 year...      1             0\n",
       "2       love deagans really atmosphere cozy festive sh...      5             1\n",
       "3       dismal lukewarm defrostedtasting texmex glopmu...      1             0\n",
       "4       oh happy day finally canes near casa yes other...      4             1\n",
       "...                                                   ...    ...           ...\n",
       "499995  kung fu tea havent amazing boba long time firs...      5             1\n",
       "499996  wish one westside case boba shoplet start taro...      5             1\n",
       "499997  new favorite spot dylan waitress absolutely fa...      5             1\n",
       "499998  35 starsenvironment decor poor floor sometimes...      3             0\n",
       "499999  want tell everyone experience tuesday evening ...      5             1\n",
       "\n",
       "[500000 rows x 3 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents is 500000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents is %s\"%len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels is 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of labels is %s\"%len(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      "    label  # of reviews  binary_label\n",
      "0      1         70459         70468\n",
      "1      2         40577         40577\n",
      "2      3         55773         55778\n",
      "3      4        112795        112802\n",
      "4      5        220354        220375\n"
     ]
    }
   ],
   "source": [
    "print(\"Label distribution:\\n\", data.groupby('label').count().reset_index().rename(columns={'text':'# of reviews'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of reviews is 108.456586\n"
     ]
    }
   ],
   "source": [
    "print(\"Average word length of reviews is %s\"%(np.mean([len(text.split(' ')) for text in texts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "# unigram\n",
    "\n",
    "# fit tfidf\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=500, norm='l2',\n",
    "                    max_features=500, encoding='UTF-8',\n",
    "                     ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "\n",
    "# get tfidf features in a sparse matrix\n",
    "fe1 = tfidf.fit_transform(data['text'].tolist())\n",
    "\n",
    "# turn into a dataframe of features\n",
    "fe_df1 = pd.DataFrame.sparse.from_spmatrix(fe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(fe1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1gram+2gram\n",
    "\n",
    "# fit tfidf\n",
    "tfidf2 = TfidfVectorizer(sublinear_tf=True, min_df=500, norm='l2',\n",
    "                     max_features=500, encoding='UTF-8',\n",
    "                     ngram_range=(1,2), stop_words='english')\n",
    "\n",
    "# get tfidf features in a sparse matrix\n",
    "fe2 = tfidf2.fit_transform(data['text'].tolist())\n",
    "\n",
    "# turn into a dataframe of features\n",
    "fe_df2 = pd.DataFrame.sparse.from_spmatrix(fe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(fe2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(fe_df1, data['binary_label'], test_size=0.3, random_state=66)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(fe_df2, data['binary_label'], test_size=0.3, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8589\n",
      "Precision: 0.8792\n",
      "Recall: 0.9138\n",
      "F1 score: 0.8962\n",
      "Micro-averaged F1 score: 0.8589\n"
     ]
    }
   ],
   "source": [
    "# unigram - default\n",
    "\n",
    "# train a logistic regression model on tfidf features - default\n",
    "lr_unigram = LogisticRegression(random_state=66)\n",
    "lr_unigram.fit(X_train1, y_train1)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_unigram = lr_unigram.predict(X_test1)\n",
    "\n",
    "# evaluation metrics\n",
    "print(\"Accuracy: %0.4f\"%accuracy_score(y_test1, y_pred_unigram))\n",
    "print(\"Precision: %0.4f\"%precision_score(y_test1, y_pred_unigram))\n",
    "print(\"Recall: %0.4f\"%recall_score(y_test1, y_pred_unigram))\n",
    "print(\"F1 score: %0.4f\"%f1_score(y_test1, y_pred_unigram))\n",
    "print(\"Micro-averaged F1 score: %0.4f\"%f1_score(y_test1, y_pred_unigram, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8590\n",
      "Precision: 0.8791\n",
      "Recall: 0.9142\n",
      "F1 score: 0.8963\n",
      "Micro-averaged F1 score: 0.8590\n"
     ]
    }
   ],
   "source": [
    "# unigram - regularization and stopping criterion\n",
    "\n",
    "# train a logistic regression model with regularization and stopping criteria on tfidf features\n",
    "lr_unigram2 = LogisticRegression(C=0.6, random_state=66)\n",
    "lr_unigram2.fit(X_train1, y_train1)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_unigram2 = lr_unigram2.predict(X_test1)\n",
    "\n",
    "# evaluation metrics\n",
    "print(\"Accuracy: %0.4f\"%accuracy_score(y_test1, y_pred_unigram2))\n",
    "print(\"Precision: %0.4f\"%precision_score(y_test1, y_pred_unigram2))\n",
    "print(\"Recall: %0.4f\"%recall_score(y_test1, y_pred_unigram2))\n",
    "print(\"F1 score: %0.4f\"%f1_score(y_test1, y_pred_unigram2))\n",
    "print(\"Micro-averaged F1 score: %0.4f\"%f1_score(y_test1, y_pred_unigram2, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8588\n",
      "Precision: 0.8794\n",
      "Recall: 0.9135\n",
      "F1 score: 0.8961\n",
      "Micro-averaged F1 score: 0.8588\n"
     ]
    }
   ],
   "source": [
    "# 1gram+2gram - default\n",
    "\n",
    "# train logistic model\n",
    "lr_2gram = LogisticRegression(random_state=66)\n",
    "lr_2gram.fit(X_train2, y_train2)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_2gram = lr_2gram.predict(X_test2)\n",
    "\n",
    "# evaluation metrics\n",
    "print(\"Accuracy: %0.4f\"%accuracy_score(y_test2, y_pred_2gram))\n",
    "print(\"Precision: %0.4f\"%precision_score(y_test2, y_pred_2gram))\n",
    "print(\"Recall: %0.4f\"%recall_score(y_test2, y_pred_2gram))\n",
    "print(\"F1 score: %0.4f\"%f1_score(y_test2, y_pred_2gram))\n",
    "print(\"Micro-averaged F1 score: %0.4f\"%f1_score(y_test2, y_pred_2gram, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8589\n",
      "Precision: 0.8792\n",
      "Recall: 0.9138\n",
      "F1 score: 0.8962\n",
      "Micro-averaged F1 score: 0.8589\n"
     ]
    }
   ],
   "source": [
    "# 1gram+2gram - regularization and stopping criterion\n",
    "\n",
    "# train logistic model \n",
    "lr_2gram2 = LogisticRegression(C=0.6, random_state=66)\n",
    "lr_2gram2.fit(X_train2, y_train2)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_2gram2 = lr_2gram2.predict(X_test2)\n",
    "\n",
    "# evaluation metrics\n",
    "print(\"Accuracy: %0.4f\"%accuracy_score(y_test2, y_pred_2gram2))\n",
    "print(\"Precision: %0.4f\"%precision_score(y_test2, y_pred_2gram2))\n",
    "print(\"Recall: %0.4f\"%recall_score(y_test2, y_pred_2gram2))\n",
    "print(\"F1 score: %0.4f\"%f1_score(y_test2, y_pred_2gram2))\n",
    "print(\"Micro-averaged F1 score: %0.4f\"%f1_score(y_test2, y_pred_2gram2, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8588\n",
      "Precision: 0.8780\n",
      "Recall: 0.9154\n",
      "F1 score: 0.8963\n",
      "Micro-averaged F1 score: 0.8588\n"
     ]
    }
   ],
   "source": [
    "# unigram - default\n",
    "\n",
    "# train SVM model on tfidf features - default\n",
    "svm_unigram = LinearSVC(random_state=66)\n",
    "svm_unigram.fit(X_train1, y_train1)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_unigram = svm_unigram.predict(X_test1)\n",
    "\n",
    "# evaluation metrics\n",
    "print(\"Accuracy: %0.4f\"%accuracy_score(y_test1, y_pred_unigram))\n",
    "print(\"Precision: %0.4f\"%precision_score(y_test1, y_pred_unigram))\n",
    "print(\"Recall: %0.4f\"%recall_score(y_test1, y_pred_unigram))\n",
    "print(\"F1 score: %0.4f\"%f1_score(y_test1, y_pred_unigram))\n",
    "print(\"Micro-averaged F1 score: %0.4f\"%f1_score(y_test1, y_pred_unigram, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8595\n",
      "Precision: 0.8781\n",
      "Recall: 0.9164\n",
      "F1 score: 0.8969\n",
      "Micro-averaged F1 score: 0.8595\n"
     ]
    }
   ],
   "source": [
    "# unigram - penalty\n",
    "\n",
    "# train SVM model \n",
    "svm_unigram2 = LinearSVC(random_state=66, loss='hinge', C=10)\n",
    "svm_unigram2.fit(X_train1, y_train1)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_unigram2 = svm_unigram2.predict(X_test1)\n",
    "\n",
    "# evaluation metrics\n",
    "print(\"Accuracy: %0.4f\"%accuracy_score(y_test1, y_pred_unigram2))\n",
    "print(\"Precision: %0.4f\"%precision_score(y_test1, y_pred_unigram2))\n",
    "print(\"Recall: %0.4f\"%recall_score(y_test1, y_pred_unigram2))\n",
    "print(\"F1 score: %0.4f\"%f1_score(y_test1, y_pred_unigram2))\n",
    "print(\"Micro-averaged F1 score: %0.4f\"%f1_score(y_test1, y_pred_unigram2, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8592\n",
      "Precision: 0.8787\n",
      "Recall: 0.9150\n",
      "F1 score: 0.8965\n",
      "Micro-averaged F1 score: 0.8592\n"
     ]
    }
   ],
   "source": [
    "# 1gram+2gram - default \n",
    "\n",
    "# train SVM with penalty\n",
    "svm_2gram1 = LinearSVC(random_state=66)\n",
    "svm_2gram1.fit(X_train2, y_train2)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_2gram1 = svm_2gram1.predict(X_test2)\n",
    "\n",
    "# evaluation metrics\n",
    "print(\"Accuracy: %0.4f\"%accuracy_score(y_test2, y_pred_2gram1))\n",
    "print(\"Precision: %0.4f\"%precision_score(y_test2, y_pred_2gram1))\n",
    "print(\"Recall: %0.4f\"%recall_score(y_test2, y_pred_2gram1))\n",
    "print(\"F1 score: %0.4f\"%f1_score(y_test2, y_pred_2gram1))\n",
    "print(\"Micro-averaged F1 score: %0.4f\"%f1_score(y_test2, y_pred_2gram1, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8595\n",
      "Precision: 0.8786\n",
      "Recall: 0.9156\n",
      "F1 score: 0.8968\n",
      "Micro-averaged F1 score: 0.8595\n"
     ]
    }
   ],
   "source": [
    "# 1gram+2gram - regularization and stopping criterion\n",
    "\n",
    "# train SVM with penalty\n",
    "svm_2gram2 = LinearSVC(random_state=66, loss='hinge', C=10)\n",
    "svm_2gram2.fit(X_train2, y_train2)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred_2gram2 = svm_2gram2.predict(X_test2)\n",
    "\n",
    "# evaluation metrics\n",
    "print(\"Accuracy: %0.4f\"%accuracy_score(y_test2, y_pred_2gram2))\n",
    "print(\"Precision: %0.4f\"%precision_score(y_test2, y_pred_2gram2))\n",
    "print(\"Recall: %0.4f\"%recall_score(y_test2, y_pred_2gram2))\n",
    "print(\"F1 score: %0.4f\"%f1_score(y_test2, y_pred_2gram2))\n",
    "print(\"Micro-averaged F1 score: %0.4f\"%f1_score(y_test2, y_pred_2gram2, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf.pkl']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save best performing svm model\n",
    "joblib.dump(svm_unigram2, 'svm_best.pkl')\n",
    "\n",
    "# save the tfidf vectorizer\n",
    "joblib.dump(tfidf, 'tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vec.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "pickle.dump(svm_unigram2, open('yelp_svm.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /Users/zazhu/.venv/py37/lib/python3.7/site-packages (from fasttext) (2.5.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/zazhu/.venv/py37/lib/python3.7/site-packages (from fasttext) (39.1.0)\n",
      "Requirement already satisfied: numpy in /Users/zazhu/.venv/py37/lib/python3.7/site-packages (from fasttext) (1.19.1)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-macosx_10_13_x86_64.whl size=324173 sha256=61e6d17bc7af2c71643949bdcf78291ae81b81204321f9955d8d8efc5300e859\n",
      "  Stored in directory: /Users/zazhu/Library/Caches/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/zazhu/.venv/py37/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install fasttext\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "# fasttext requires data to be in the format of: __label__1 text\n",
    "data['new_label'] = data.apply(lambda t: '__label__' + str(t['binary_label']) + ' ' + str(t['text']),\n",
    "                           axis=1)\n",
    "# train test split\n",
    "X = data['new_label']\n",
    "y = data['binary_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train and test data\n",
    "X_train.to_csv('fasttext_train.txt',index=False, header=False)\n",
    "X_test.to_csv('fasttext_test.txt',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9015\n",
      "Recall: 0.9015\n",
      "F1 score: 0.9015\n"
     ]
    }
   ],
   "source": [
    "# fasttext model - default\n",
    "ft_model = fasttext.train_supervised('fasttext_train.txt')\n",
    "\n",
    "# calculate evaluation metrics\n",
    "result = ft_model.test('fasttext_test.txt')\n",
    "precision = result[1]\n",
    "recall = result[2]\n",
    "print(\"Precision: %0.4f\"%precision)\n",
    "print(\"Recall: %0.4f\"%recall)\n",
    "print(\"F1 score: %0.4f\"%(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9116\n",
      "Recall: 0.9116\n",
      "F1 score: 0.9116\n"
     ]
    }
   ],
   "source": [
    "# fasttext model - setting 1\n",
    "ft_model = fasttext.train_supervised('fasttext_train.txt',wordNgrams=2)\n",
    "\n",
    "# calculate evaluation metrics\n",
    "result = ft_model.test('fasttext_test.txt')\n",
    "precision = result[1]\n",
    "recall = result[2]\n",
    "print(\"Precision: %0.4f\"%precision)\n",
    "print(\"Recall: %0.4f\"%recall)\n",
    "print(\"F1 score: %0.4f\"%(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9066\n",
      "Recall: 0.9066\n",
      "F1 score: 0.9066\n"
     ]
    }
   ],
   "source": [
    "# fasttext model - setting 2\n",
    "ft_model = fasttext.train_supervised('fasttext_train.txt',lr=0.8, wordNgrams=2)\n",
    "\n",
    "# calculate evaluation metrics\n",
    "result = ft_model.test('fasttext_test.txt')\n",
    "precision = result[1]\n",
    "recall = result[2]\n",
    "print(\"Precision: %0.4f\"%precision)\n",
    "print(\"Recall: %0.4f\"%recall)\n",
    "print(\"F1 score: %0.4f\"%(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9110\n",
      "Recall: 0.9110\n",
      "F1 score: 0.9110\n"
     ]
    }
   ],
   "source": [
    "# fasttext model - setting 3\n",
    "ft_model = fasttext.train_supervised('fasttext_train.txt',lr=0.05, epoch=10, wordNgrams=2)\n",
    "\n",
    "# calculate evaluation metrics\n",
    "result = ft_model.test('fasttext_test.txt')\n",
    "precision = result[1]\n",
    "recall = result[2]\n",
    "print(\"Precision: %0.4f\"%precision)\n",
    "print(\"Recall: %0.4f\"%recall)\n",
    "print(\"F1 score: %0.4f\"%(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SVM model to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"predict_svm.ipynb\n",
    "Automatically generated by Colaboratory.\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1C_Wz45F6ilIY7nV4pbBi_dnpeT0gFaAr\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = pickle.load(open('yelp_svm.sav', 'rb'))\n",
    "\n",
    "with open('tfidf_vec.pkl', 'rb') as f:\n",
    "    feature_transformer = pickle.load(f)\n",
    "\n",
    "review = ['I love this restaurant! It is sooooo good!',\n",
    "         'The delivery never came. I had to call them to cancel the order.',\n",
    "         'The place is nice with large space and nice decoration.',\n",
    "         'The food is okay but too pricy',\n",
    "         'It is easy to park nearby, furnished recently, but too many people in the gym.']\n",
    "\n",
    "label = [1, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and normalize the documents\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# convert to dataframe\n",
    "data = pd.DataFrame({'text': review, 'label': label})\n",
    "\n",
    "# keep only numbers, letters and space\n",
    "data['text'] = data.apply(lambda t: re.sub(r'[^0-9A-Za-z ]', '', str(t['text'])), axis=1)\n",
    "\n",
    "# remove stopwords and convert to lower case\n",
    "data['text'] = data.apply(lambda r: ' '.join(w.lower() for w in r['text'].split() if w.lower() not in stop_words),axis=1)\n",
    "\n",
    "# discard NA reviews\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love restaurant sooooo good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delivery never came call cancel order</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>place nice large space nice decoration</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food okay pricy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>easy park nearby furnished recently many peopl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                        love restaurant sooooo good      1\n",
       "1              delivery never came call cancel order      0\n",
       "2             place nice large space nice decoration      1\n",
       "3                                    food okay pricy      0\n",
       "4  easy park nearby furnished recently many peopl...      0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = feature_transformer.transform(data['text'].tolist())\n",
    "\n",
    "confidence_score = model_svm.decision_function(fe)\n",
    "y_pred = model_svm.predict(fe)\n",
    "\n",
    "result = {}\n",
    "for i in range(len(data)):\n",
    "    result[str(i) + '__label: ' + str(label[i])] = {'predicted label': int(y_pred[i]),\n",
    "                             'confidence score': confidence_score[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"0__label: 1\": {\n",
      "    \"predicted label\": 1,\n",
      "    \"confidence score\": 2.696785887134676\n",
      "  },\n",
      "  \"1__label: 0\": {\n",
      "    \"predicted label\": 0,\n",
      "    \"confidence score\": -0.1881717078405898\n",
      "  },\n",
      "  \"2__label: 1\": {\n",
      "    \"predicted label\": 1,\n",
      "    \"confidence score\": 1.1902715709202243\n",
      "  },\n",
      "  \"3__label: 0\": {\n",
      "    \"predicted label\": 0,\n",
      "    \"confidence score\": -3.8353964250034984\n",
      "  },\n",
      "  \"4__label: 0\": {\n",
      "    \"predicted label\": 1,\n",
      "    \"confidence score\": 1.0917192647405367\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svm_predcition.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
